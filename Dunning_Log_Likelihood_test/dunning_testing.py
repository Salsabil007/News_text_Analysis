# -*- coding: utf-8 -*-
"""dunning_testing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OLmbObdOiNYyZwjNbEJjPnI2R238r6kV
"""

import pandas as pd
import re
import nltk
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
#from flair.models import TARSTagger
#from flair.data import Sentence
from nltk import pos_tag
from nltk.tokenize import RegexpTokenizer

import sys
from google.colab import drive
drive.mount('/content/drive')

sys.path.append('/content/drive/My Drive/news_text_analysis/')

data = pd.read_csv("/content/drive/My Drive/news_text_analysis/data_dunning_jan15.csv")
print(data.dtypes)

!pip install flair

df = pd.DataFrame()
df['clean_comments'] = ["Samuel is a good guy. I like him","I read in University of wisconsin, Madison"]
df['name'] = ['Shusmi',"Mou"]

tars = TARSTagger.load('tars-ner')
labels = ['Person','University']
tars.add_and_switch_to_new_task('task 1', labels, label_type='ner')
no_ner = []
for comment in df.clean_comments.values.tolist():
    sentence = Sentence(comment)
    #sentence = Sentence(' '.join(comment))

    tars.predict(sentence)
    for entity in sentence.get_spans('ner'):
      print("entity ",entity)
    #print("sentence ",sentence)
    temp = sentence.to_tagged_string("ner").split(' ')
    print("temp ",temp)
    ind = [i for i, item in enumerate(temp) if re.search('<[A-Z]-Person>|<[A-Z]-University>', item)]
    print("ind ",ind)
    ind += [y-1 for y in ind]
    for index in sorted(ind, reverse=True):
        del temp[index]
    no_ner.append(' '.join(temp))
df['no_ner'] = no_ner

df.iloc[0]['no_ner']



from flair.data import Sentence
from flair.models import SequenceTagger

# load tagger
tagger = SequenceTagger.load("flair/ner-english")

# make example sentence
sentence = Sentence("Samuel is a good guy. I like him")

# predict NER tags
tagger.predict(sentence)

# print sentence
print(sentence)

# print predicted NER spans
print('The following NER tags are found:')
# iterate over entities and print
for entity in sentence.get_spans('ner'):
    print(entity)

import pandas as pd
import numpy as np
import nltk
from nltk import pos_tag
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
import collections
from nltk.corpus import stopwords
nltk.download('stopwords')

import re

def remove_punctuation(text):
    # Define a regular expression to match punctuation
    punctuation_pattern = re.compile(r'[.,:;/"\']')

    # Use the regular expression to replace punctuation with an empty string
    text_without_punctuation = punctuation_pattern.sub('', text)

    return text_without_punctuation


data = pd.DataFrame()
data['text'] = ["I am extremely Happy. I am truely very happy. She said, \"X\".","They are very nice people"]
data['name'] = ["Shusmi","Salsabil"]

adj = ["JJ","JJR","JJS"]
stopset = set(stopwords.words('english'))

TOKENIZER = RegexpTokenizer('(?u)\W+|\$[\d\.]+|\S+')

data['text'] = data['text'].apply(lambda x: remove_punctuation(x))
data['token'] = data['text'].apply(lambda x: nltk.word_tokenize(x.lower()))

#data['token'] = data['text'].apply(lambda x: TOKENIZER.tokenize(x.lower()))

data['token2'] = data['token'].apply(lambda x: [word for word in x if word not in stopset])
data['pos_tag'] = data['token2'].apply(lambda x: pos_tag(x))
data['adj'] = data['pos_tag'].apply(lambda x: [word for word,pos in x if pos in adj])

print(data)

merged_adj = data['adj'].explode().tolist()
print(merged_adj)
c = collections.Counter(merged_adj)
print(c)

import pandas as pd

# Sample DataFrame
data = {'column_with_lists': [[1, 2, 3], [4, 5,3], [6, 7, 8, 9]]}
df = pd.DataFrame(data)

# Merge lists across rows into a single list
merged_list = df['column_with_lists'].explode().tolist()

print(merged_list)

import math
from collections import Counter

import nltk
from scipy.stats import chi2

def dunn_individual_word(total_words_in_corpus_1, total_words_in_corpus_2,
                         count_of_word_in_corpus_1,
                         count_of_word_in_corpus_2):
    '''
    applies dunning log likelihood to compare individual word in two counter objects

    :param word: desired word to compare
    :param m_corpus: c.filter_by_gender('male')
    :param f_corpus: c. filter_by_gender('female')
    :return: log likelihoods and p value
    >>> total_words_m_corpus = 8648489
    >>> total_words_f_corpus = 8700765
    >>> wordcount_female = 1000
    >>> wordcount_male = 50
    >>> dunn_individual_word(total_words_m_corpus,total_words_f_corpus,wordcount_male,wordcount_female)
    -1047.8610274053995
    '''
    a = count_of_word_in_corpus_1
    b = count_of_word_in_corpus_2
    c = total_words_in_corpus_1
    d = total_words_in_corpus_2

    e1 = c * (a + b) / (c + d)
    e2 = d * (a + b) / (c + d)

    dunning_log_likelihood = 2 * (a * math.log(a / e1) + b * math.log(b / e2))

    if count_of_word_in_corpus_1 * math.log(count_of_word_in_corpus_1 / e1) < 0:
        dunning_log_likelihood = -dunning_log_likelihood

    p = 1 - chi2.cdf(abs(dunning_log_likelihood),1)

    return dunning_log_likelihood


def dunning_total(counter1, counter2, filename_to_pickle=None):
    '''
    runs dunning_individual on words shared by both counter objects
    (-) end of spectrum is words for counter_2
    (+) end of spectrum is words for counter_1
    the larger the magnitude of the number, the more distinctive that word is in its
    respective counter object

    use filename_to_pickle to store the result so it only has to be calculated once and can be
    used for multiple analyses.

    >>> from collections import Counter
    >>> female_counter = Counter({'he': 1,  'she': 10, 'and': 10})
    >>> male_counter =   Counter({'he': 10, 'she': 1,  'and': 10})
    >>> results = dunning_total(female_counter, male_counter)

    # Results is a dict that maps from terms to results
    # Each result dict contains the dunning score...
    >>> results['he']['dunning']
    -8.547243830635558

    # ... counts for corpora 1 and 2 as well as total count
    >>> results['he']['count_total'], results['he']['count_corp1'], results['he']['count_corp2']
    (11, 1, 10)

    # ... and the same for frequencies
    >>> results['he']['freq_total'], results['he']['freq_corp1'], results['he']['freq_corp2']
    (0.2619047619047619, 0.047619047619047616, 0.47619047619047616)

    :return: dict

    '''

    total_words_counter1 = 0
    total_words_counter2 = 0

    #get word total in respective counters
    for word1 in counter1:
        total_words_counter1 += counter1[word1]
    for word2 in  counter2:
        total_words_counter2 += counter2[word2]

    #dictionary where results will be returned
    dunning_result = {}
    for word in counter1:
        counter1_wordcount = counter1[word]
        if word in counter2:
            counter2_wordcount = counter2[word]


            if counter1_wordcount + counter2_wordcount < 10:
                continue

            dunning_word = dunn_individual_word( total_words_counter1,  total_words_counter2,
                                                 counter1_wordcount,counter2_wordcount)

            dunning_result[word] = {
                'dunning': dunning_word,
                'count_total': counter1_wordcount + counter2_wordcount,
                'count_corp1': counter1_wordcount,
                'count_corp2': counter2_wordcount,
                'freq_total': (counter1_wordcount + counter2_wordcount) / (total_words_counter1 +
                                                                           total_words_counter2),
                'freq_corp1': counter1_wordcount / total_words_counter1,
                'freq_corp2': counter2_wordcount / total_words_counter2
            }

    if filename_to_pickle:
        store_pickle(dunning_result, filename_to_pickle)

    return dunning_result

female_counter = Counter({'attractive': 10,  'handsome': 1, 'nice': 15, 'stylish': 20, 'cool': 3, 'awesome': 1,'wise':7, 'hot': 12,"bingo": 2})
male_counter =   Counter({'handsome': 10, 'nice': 5, 'stylish': 2, 'cool': 13, 'awesome': 15,'wise':11, 'hot': 6, 'attractive': 2})
results = dunning_total(male_counter, female_counter)
#print(results['he']['dunning'])
#print(results['she']['dunning'])
#print(results['yes']['dunning'])
print(results)

#print(results[0])

dunning_scores = {word: info['dunning'] for word, info in results.items()}

# Display Dunning scores
for word, score in dunning_scores.items():
    print(f'Word: {word}, Dunning Score: {score}')

# Sorting by Dunning scores in descending order (highest first) for male
sorted_llr_male = sorted(dunning_scores.items(), key=lambda x: x[1], reverse=True)[:5]

# Sorting by Dunning scores in ascending order (lowest first) for female
sorted_llr_female = sorted(dunning_scores.items(), key=lambda x: x[1])[:5]

sorted_llr_male

sorted_llr_female

"""## running dunning on data"""

#confusions
#should I select similar/close number of data instances for both gender?
#top 30 - is it top 30 based on the frequency or based on the dunning score?
#https://stackoverflow.com/questions/13529945/pos-tagging-nltk-thinks-noun-is-adjective
#try new way to recognize entity
#find a way to identify and remove headlines
#remove words that appear in the title to remove some topic related words

data = pd.read_csv("/content/drive/My Drive/news_text_analysis/data_dunning_jan15.csv")
print(data.dtypes)
print(len(data))

#data2 = data2.dropna(subset = ['gender'])
data["G"] = data["gender"].apply(lambda x: "Female" if (x == "F" or x == "female") else "Male")
print(len(data[data['G'] == "Female"]), " ",len(data[data['G'] == "Male"]))




# Assuming your DataFrame is named 'df'
# Replace 'gender_column' with the actual column name containing gender information

# Separate the DataFrame into two based on gender
male_data = data[data['G'] == 'Male']
female_data = data[data['G'] == 'Female']

# Determine the smaller size between male and female datasets
min_size = min(len(male_data), len(female_data))

# Sample a subset from each gender category
male_sampled = male_data.sample(n=min_size, random_state=42)  # Use a fixed random state for reproducibility
female_sampled = female_data.sample(n=min_size, random_state=42)

# Concatenate the sampled DataFrames to create the final balanced dataset
balanced_data = pd.concat([male_sampled, female_sampled])

data = balanced_data.copy(deep = True)
print(len(data[data['G'] == "Female"]), " ",len(data[data['G'] == "Male"]))

def remove_punctuation(text):
    # Define a regular expression to match punctuation
    punctuation_pattern = re.compile(r'[.,:;|/"\']')

    # Use the regular expression to replace punctuation with an empty string
    text_without_punctuation = punctuation_pattern.sub('', text)

    return text_without_punctuation

adj = ["JJ","JJR","JJS"]
stopset = set(stopwords.words('english'))

data['new_text'] = data['new_text'].apply(lambda x: remove_punctuation(x))
data['token'] = data['new_text'].apply(lambda x: nltk.word_tokenize(x.lower()))
data['token2'] = data['token'].apply(lambda x: [word for word in x if word not in stopset and word not in ['x','’','u.','u','subscribe','copyright','attachment','202-413-4226view',
                                                                                                           "advertise","advertisement","thanks","thank","paywall"]])
data['pos_tag'] = data['token2'].apply(lambda x: pos_tag(x))
data['adj'] = data['pos_tag'].apply(lambda x: [word for word,pos in x if pos in adj])

print(data.head(10))

#merged_adj = data['adj'].explode().tolist()
#print(merged_adj)
#c = collections.Counter(merged_adj)
#print(c)

data2 = data.copy(deep = True)


male_adj = data2.loc[data2['G'] == "Male", "adj"].explode().tolist()
female_adj = data2.loc[data2['G'] == "Female", "adj"].explode().tolist()

c_male = collections.Counter(male_adj)
c_female = collections.Counter(female_adj)

print(c_male)

print(c_female)

results = dunning_total(c_male, c_female)
dunning_scores = {word: info['dunning'] for word, info in results.items()}
freq_female =  {word: info['freq_corp2'] for word, info in results.items()}
freq_male =  {word: info['freq_corp1'] for word, info in results.items()}

# Sorting by Dunning scores in descending order (highest first) for male
sorted_llr_male = sorted(dunning_scores.items(), key=lambda x: x[1], reverse=True)[:50]

# Sorting by Dunning scores in ascending order (lowest first) for female
sorted_llr_female = sorted(dunning_scores.items(), key=lambda x: x[1])[:50]

i = 0
d1,d2, d3 = [],[],[]
while i < len(sorted_llr_male):
  d1.append(sorted_llr_male[i][0])
  d2.append(sorted_llr_male[i][1])
  d3.append(freq_male[sorted_llr_male[i][0]])
  i += 1
maled = pd.DataFrame()
maled['word'] = d1
maled['dunning'] = d2
maled['freq'] = d3
print(maled.head(50))

i = 0
d1,d2, d3 = [],[],[]
while i < len(sorted_llr_female):
  d1.append(sorted_llr_female[i][0])
  d2.append(sorted_llr_female[i][1])
  d3.append(freq_female[sorted_llr_female[i][0]])
  i += 1
femaled = pd.DataFrame()
femaled['word'] = d1
femaled['dunning'] = d2
femaled['freq'] = d3
print(femaled.head(50))

sorted_llr_female

dunning_scores['bipolar']

xx = pd.DataFrame()
xx['name'] = [0,1,0,1,0,1]
xx['gen'] = ["m","F","m","m","f","female"]
xx['lst'] = [[0,1],[1,2],[2,3],[1,3],[1,4],[4,5]]

xx["G"] = xx["gen"].apply(lambda x: "Female" if (x == "f" or x == "F" or x == "female") else "Male")
print(xx)

merged_adj = xx.loc[xx['G'] == "Male", "lst"].explode().tolist()     #data['adj'].explode().tolist()
print(merged_adj)

import re

def detect_headlines(text):
    # Split the text into lines
    lines = text.split('\n')

    # Define a regular expression pattern to match potential headlines
    headline_pattern = re.compile(r'^[A-Z][^.?!;:]*$')

    # Identify lines that match the pattern
    headlines = [line.strip() for line in lines if headline_pattern.match(line)]

    return headlines

# Example usage
news_text = """
This is a regular news article.
Breaking News: Important Event Happening Now!
Another regular news paragraph.
Analysis
World Leaders Gather for Summit.
"""

detected_headlines = detect_headlines(news_text)

# Print the detected headlines
for headline in detected_headlines:
    print(headline)

data.iloc[1]['new_text']

"""Approach 2"""

from textblob import TextBlob
def get_adjectives(text):
    blob = TextBlob(text)
    return [ word.lower() for (word,tag) in blob.tags if tag.startswith("JJ")]

data = pd.DataFrame()
data['text'] = ["I am extremely Happy. I am truely very happy. She said, \"X\".","They are very nice people"]
data['name'] = ["Shusmi","Salsabil"]

data['adjectives'] = data['text'].apply(get_adjectives)

print(data)

data = pd.read_csv("/content/drive/My Drive/news_text_analysis/data_dunning_jan15.csv")
print(data.dtypes)
print(len(data))

#data2 = data2.dropna(subset = ['gender'])
data["G"] = data["gender"].apply(lambda x: "Female" if (x == "F" or x == "female") else "Male")
print(len(data[data['G'] == "Female"]), " ",len(data[data['G'] == "Male"]))




# Assuming your DataFrame is named 'df'
# Replace 'gender_column' with the actual column name containing gender information

# Separate the DataFrame into two based on gender
male_data = data[data['G'] == 'Male']
female_data = data[data['G'] == 'Female']

# Determine the smaller size between male and female datasets
min_size = min(len(male_data), len(female_data))

# Sample a subset from each gender category
male_sampled = male_data.sample(n=min_size, random_state=42)  # Use a fixed random state for reproducibility
female_sampled = female_data.sample(n=min_size, random_state=42)

# Concatenate the sampled DataFrames to create the final balanced dataset
balanced_data = pd.concat([male_sampled, female_sampled])

data = balanced_data.copy(deep = True)
print(len(data[data['G'] == "Female"]), " ",len(data[data['G'] == "Male"]))

def remove_punctuation(text):
    # Define a regular expression to match punctuation
    punctuation_pattern = re.compile(r'[.,:;|/"\']')

    # Use the regular expression to replace punctuation with an empty string
    text_without_punctuation = punctuation_pattern.sub('', text)

    return text_without_punctuation

adj = ["JJ","JJR","JJS"]
stopset = set(stopwords.words('english'))


data['new_text'] = data['new_text'].apply(lambda x: remove_punctuation(x))
data['adj'] = data['new_text'].apply(get_adjectives)
'''data['token'] = data['new_text'].apply(lambda x: nltk.word_tokenize(x.lower()))
data['token2'] = data['token'].apply(lambda x: [word for word in x if word not in stopset and word not in ['x','’','u.','u','subscribe','copyright','attachment','202-413-4226view',
                                                                                                           "advertise","advertisement","thanks","thank","paywall"]])
data['pos_tag'] = data['token2'].apply(lambda x: pos_tag(x))
data['adj'] = data['pos_tag'].apply(lambda x: [word for word,pos in x if pos in adj])'''

print(data.head(10))

data2 = data.copy(deep = True)


male_adj = data2.loc[data2['G'] == "Male", "adj"].explode().tolist()
female_adj = data2.loc[data2['G'] == "Female", "adj"].explode().tolist()

c_male = collections.Counter(male_adj)
c_female = collections.Counter(female_adj)

results = dunning_total(c_male, c_female)
dunning_scores = {word: info['dunning'] for word, info in results.items()}
freq_female =  {word: info['freq_corp2'] for word, info in results.items()}
freq_male =  {word: info['freq_corp1'] for word, info in results.items()}
cnt_male = {word: info['count_corp1'] for word, info in results.items()}
cnt_female = {word: info['count_corp2'] for word, info in results.items()}

# Sorting by Dunning scores in descending order (highest first) for male
sorted_llr_male = sorted(dunning_scores.items(), key=lambda x: x[1], reverse=True)[:50]

# Sorting by Dunning scores in ascending order (lowest first) for female
sorted_llr_female = sorted(dunning_scores.items(), key=lambda x: x[1])[:50]

print(sorted_llr_male)

i = 0
d1,d2, d3, d4 = [],[],[], []
while i < len(sorted_llr_male):
  d1.append(sorted_llr_male[i][0])
  d2.append(sorted_llr_male[i][1])
  d3.append(freq_male[sorted_llr_male[i][0]])
  d4.append(cnt_male[sorted_llr_male[i][0]])
  i += 1
maled = pd.DataFrame()
maled['word'] = d1
maled['dunning'] = d2
maled['freq'] = d3
maled['cnt'] = d4
print(maled.head(50))

i = 0
d1,d2, d3, d4 = [],[],[],[]
while i < len(sorted_llr_female):
  d1.append(sorted_llr_female[i][0])
  d2.append(sorted_llr_female[i][1])
  d3.append(freq_female[sorted_llr_female[i][0]])
  d4.append(cnt_female[sorted_llr_female[i][0]])
  i += 1
femaled = pd.DataFrame()
femaled['word'] = d1
femaled['dunning'] = d2
femaled['freq'] = d3
femaled['cnt'] = d4
print(femaled.head(50))

data['is_emotional'] = data['adj'].apply(lambda x: 1 if "emotional" in x else 0)

xx = data[data['is_emotional'] == 1]
print(len(xx))

xx.iloc[1]['URL']

#https://cristobal-veas-ch.medium.com/artificial-intelligence-extracting-qualifying-adjectives-from-comments-on-donald-trump-facebook-84bf60be5c8e